const e=JSON.parse(`{"key":"v-2b37cb50","path":"/ai/llm/","title":"LLM","lang":"zh-CN","frontmatter":{"category":"AI","tag":["LLM"],"description":"LLM 基础 大语言模型（LLM）是通过预测下一个词的监督学习方式进行训练的。 具体来说，首先准备一个包含数百亿甚至更多词的大规模文本数据集。然后，可以从这些文本中提取句子或句子片段作为模型输入。模型会根据当前输入内容预测下一个词的概率分布。 通过不断比较模型预测和实际的下一个词，并更新模型参数最小化两者差异,语言模型逐步掌握了语言的规律，学会了预测下一个词。 基础语言模型（Base LLM）通过反复预测下一个词来训练的方式进行训练，没有明确的目标导向。 因此，如果给它一个开放式的prompt，它可能会通过自由联想生成戏剧化的内容。","head":[["meta",{"property":"og:url","content":"https://github.com/songbaicheng/songbaicheng.github.io/ai/llm/"}],["meta",{"property":"og:site_name","content":"Baicheng's Blog"}],["meta",{"property":"og:title","content":"LLM"}],["meta",{"property":"og:description","content":"LLM 基础 大语言模型（LLM）是通过预测下一个词的监督学习方式进行训练的。 具体来说，首先准备一个包含数百亿甚至更多词的大规模文本数据集。然后，可以从这些文本中提取句子或句子片段作为模型输入。模型会根据当前输入内容预测下一个词的概率分布。 通过不断比较模型预测和实际的下一个词，并更新模型参数最小化两者差异,语言模型逐步掌握了语言的规律，学会了预测下一个词。 基础语言模型（Base LLM）通过反复预测下一个词来训练的方式进行训练，没有明确的目标导向。 因此，如果给它一个开放式的prompt，它可能会通过自由联想生成戏剧化的内容。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-02-19T08:06:33.000Z"}],["meta",{"property":"article:author","content":"songbaicheng"}],["meta",{"property":"article:tag","content":"LLM"}],["meta",{"property":"article:modified_time","content":"2025-02-19T08:06:33.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"LLM\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-02-19T08:06:33.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"songbaicheng\\",\\"url\\":\\"https://github.com/songbaicheng\\",\\"email\\":\\"songbaicheng16@163.com\\"}]}"]]},"headers":[{"level":2,"title":"基础","slug":"基础","link":"#基础","children":[]},{"level":2,"title":"Tokens","slug":"tokens","link":"#tokens","children":[]}],"git":{"createdTime":1739952393000,"updatedTime":1739952393000,"contributors":[{"name":"songbaicheng","email":"songbaicheng@sdpjw.com","commits":1}]},"readingTime":{"minutes":2.29,"words":686},"filePathRelative":"ai/llm/README.md","localizedDate":"2025年2月19日","excerpt":"<h1> LLM</h1>\\n<h2> 基础</h2>\\n<p>大语言模型（LLM）是通过预测下一个词的监督学习方式进行训练的。</p>\\n<p>具体来说，首先准备一个包含数百亿甚至更多词的大规模文本数据集。然后，可以从这些文本中提取句子或句子片段作为模型输入。模型会根据当前输入内容预测下一个词的概率分布。</p>\\n<p>通过不断比较模型预测和实际的下一个词，并更新模型参数最小化两者差异,语言模型逐步掌握了语言的规律，学会了预测下一个词。</p>\\n<p>基础语言模型（Base LLM）通过反复预测下一个词来训练的方式进行训练，没有明确的目标导向。</p>\\n<p>因此，如果给它一个开放式的prompt，它可能会通过自由联想生成戏剧化的内容。</p>","autoDesc":true}`);export{e as data};
