---
category: AI
tag: 
  - 深度学习
---

# 深度学习入门

## 感知机

感知机是什么，就像电流流过导线，向前方输送电子一样，感知机的信号会形成流向前方输送信息，不过只有流（1）和不流（0）两种取值。

![基础感知机](/assets/images/ai/llm/deep-learn/基础感知机.png "基础感知机")

上面是一个接收两个输入信号的感知机的例子，其中 x1、x2 是输入信号，y 是输出信号，w1、w2
是权重，其中的圆形标志我们称为是节点或者是神经元，在输入信号被送往神经元的时候，会被乘以固定的权重然后相加（w1x1 +
w2x2），如果结果大于某个阈值（θ）则输出 1，否则输出 0，把上述内容用数学公式表示如下：

$$
y = \begin{cases}
1, \quad w_1 x_1 + w_2 x_2 \geq \theta \\
0, \quad w_1 x_1 + w_2 x_2 < \theta
\end{cases}
$$

## 与或非门

我们用感知机的思想通过代码来实现一下与或非门，然后一步一步升级变换一下，这样方便我们认识和理解感知机的一些其他概念。

首先我们从与门开始，逻辑与门需要遵循下面的规律：

| A | B | A AND B |
|---|---|---------|
| 0 | 0 | 0       |
| 0 | 1 | 0       |
| 1 | 0 | 0       |
| 1 | 1 | 1       |

根据上述规律，我们用 Python 代码实现一下与门：

```python
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1 * w1 + x2 * w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1
```

其中的参数选择完全由自己决定，只需要结果满足我们的与门定义即可，接下来我们用同样的方法实现或门和非门：

::: code-tabs#shell
@tab 或门

```python
def OR(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.3
    tmp = x1 * w1 + x2 * w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1
```

@tab 与非门

```python
def NAND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 1
    tmp = x1 * w1 + x2 * w2
    if tmp < theta:
        return 1
    elif tmp >= theta:
        return 0
```

:::

上面我们通过比较直接的方式实现了与或非门，接下来我们调整一下表达式的位置，将 θ 变成 -b 并移项到等号前，表达式就变成下面这样：

$$
y = \begin{cases}
1, \quad w_1 x_1 + w_2 x_2 + b \geq 0 \\
0, \quad w_1 x_1 + w_2 x_2 + b < 0
\end{cases}
$$

之前我们将 θ 称为阈值，可是它在新表达式中摇身一变，我们赋予他一个新的定义，叫做偏置（bias），它表示一个常数项，作用依旧是承担着神经元被激活的难易程度。我们使用权重和偏置来定义感知机，将上述三个门函数写成如下形式：

::: code-tabs#shell
@tab 与门

```python
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

@tab 或门

```python
def OR(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.3
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

@tab 与非门

```python
def NAND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([-0.5, -0.5])
    b = 0.7
    tmp = np.sum(w * x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

:::

## 感知机的局限性

到这里大家都已经如何用感知机的思想实现与或非门了，下一步我们来考虑一下异或门的问题，异或门需要满足下面的规律：

| A | B | A XOR B |
|---|---|---------|
| 0 | 0 | 0       |
| 0 | 1 | 1       |
| 1 | 0 | 1       |
| 1 | 1 | 0       |

可能这样看上去我们并没有发现有什么异常，我们先拿之前的与或非门来举个例子，比如说或门，我们将我们代码中的变量放到表达式：

$$
y = \begin{cases}
1, \quad 0.5 x_1 + 0.5 x_2 - 0.3 \geq 0 \\
0, \quad 0.5 x_1 + 0.5 x_2 - 0.3 < 0
\end{cases}
$$

我们将 x1 和 x2 作为横坐标与纵坐标作图，会得到一条直线将坐标轴分为两个空间，左侧为0，右侧为1，如下图所示：

![or 感知机可视化](/assets/images/ai/llm/deep-learning/or.png "or 感知机可视化")

图中可以看到灰色区域是我们 or 感知机输出 0 的区域，其他两种与和与非门的图类似，那现在我们尝试先用图的方式来展示一下异或门：

[//]: # (不好画，等有空画一下)

从图中我们就可以看到，异或门的实现已经不能通过一条直线分开了，所以感知机的局限就在此处，只能表示由一条直线分割的空间。这里我们引入两个重要的概念。
- 线性空间：由直线分割而成的空间。
- 非线性空间：曲线分割而成的空间。

## 多层感知机
虽然感知机只能表示线性空间，可是感知机的绝妙之处在于它可以叠加态，这里我们继续借助异或门来研究感知机的叠加态概念，异或门的制作方式有很多，其中之一就是结合我们之前的与或非门进行组合

![通过组合实现 xor](/assets/images/ai/llm/deep-learning/xor.png "通过组合实现 xor")

根据图中的组合方式，用代码也是可以同样的组合方式：

```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```

这里我们可以看出，与门、或门是单层感知机，而异或门是组合而成的 2 层感知机，这也就是感知机的叠加态，我们也能明白感知机可以通过叠加层进行非线性的表示。


## 神经网络
对于单纯的感知机虽然可以对于复杂的函数，感知机也能表示它的可能性，但是人工的设置其权重和偏置是非常繁琐的，神经网络的出现就可以自动地从数据中学习到合适的权重参数。

### 激活函数
对于如何将感知机与神经网络联系起来，我们需要引入激活函数的概念，激活函数是将输入信号的总和转换为输出信号，下图可以很好的表达出激活函数的作用：

![激活函数](/assets/images/ai/llm/deep-learning/activation-function.png "激活函数")

图中节点从 a 通过激活函数 h() 转换成 y ，我们称 a 和 y 为节点，也可以称为“神经元”，激活函数有很多种，比如阶跃函数、sigmoid 函数等。

### 损失函数
数据是机器学习的核心，我们追求的是模型泛化的能力，泛化能力是指处理未被观察过的数据的能力，只对某个数据集过度拟合的状态称为过拟合，避免过拟合是机器学习的一个重要课题。

神经网络在以某个指标为线索寻找最优参数所用的指标就是损失函数，损失函数可以是任意值，但一般使用均方误差和交叉熵误差等。

### 梯度
梯度是损失函数对参数的偏导数，梯度的方向指向损失函数的减小最快的方向，梯度下降法就是沿着这个方向不断更新权重和偏置。

### 反向传播算法
反向传播算法是神经网络的核心算法之一，它通过计算损失函数相对于每个权重的梯度来更新网络的权重和偏置，从而实现模型的训练。

## 技巧
神经网络学习的目的是找到是损失函数的值尽可能小的参数，这个过程为最优化，在神经网络的学习中，权重的初始值特别重要，如果想要减少权重的值，一开始就将初始化的值设置为较小的值才是正途。

## 卷积
卷积神经网络（CNN）是深度学习领域中一种非常重要的网络结构，它主要用于处理图像、语音识别等场合。

卷积在之前的神经网络的连接层中增加了卷积层，普通连接层在处理多维数据的时候会把多维数据转化为一维数据，而卷积层则可以保持数据的维度不变。

## 深度学习
只需要通过叠加态就可以创建深度网络，随着大数据和网络的大规模化，深度学习也开始高速化，最近的框架也开始支持多个 GPU 在多台机器上进行分布式学习。
