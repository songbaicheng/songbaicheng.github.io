# 走进 AI
AI (artificial intelligence) 人工智能，AI领域非常广泛，包含了机器学习、强化学习、专家系统等多个子领域，但 **计算机视觉（CV）** 和 **自然语言处理（NLP）** 无疑是AI的两个主领域，因为它们直接涉及到人类与计算机的互动，并且在技术进步和商业化方面都取得了巨大成就。从名字就不难看出，CV 领域研究的是生物领域，而 NLP 领域研究则是偏向心理方向。

由于自己也是逐渐摸索，没有固定的学习路线，所以就按照自己了解的知识开始进行探索。目前在工作和生活中遇到场景大部分都是 NLP 领域相关的，也就是我们常见的 AIGC（Artificial Intelligence Generated Content），是指通过 人工智能（AI）技术生成的内容，它涵盖了由AI模型（如语言模型、图像生成模型、视频生成模型等）自动生成的各种形式的内容，包括但不限于文本、图像、音频、视频等，也就是我们常说的文生图、文生文、文生音、图生图等。

学习 NLP 要抓住一个核心两个工具，分别是 **Hugging Face**、**PEFT** 和 **LangChain** ，它们是当前在 自然语言处理（NLP） 和 生成式 AI 领域中非常流行和关键的框架或工具，首先我们先来介绍一下它们呢。

## Hugging Face
Hugging Face 是一个开源社区和平台，提供了大量预训练的 transformer 模型，如 BERT、GPT、T5、BLOOM 等，并支持自然语言处理（NLP）的各种任务（如文本分类、命名实体识别、机器翻译、文本生成等）。
- 提供了 transformers 库，简化了大规模预训练模型的使用，使得开发者可以快速加载、微调并部署各种模型。
- 拥有强大的 datasets 库，允许快速访问和加载各种 NLP 任务的数据集。
- Hugging Face Hub 提供了一个模型共享平台，任何人都可以上传和下载模型，极大地推动了AI模型的开源共享。

对于NLP的开发者和研究人员来说，Hugging Face 提供了几乎所有你需要的资源，是开发和应用生成式AI模型、预训练模型微调的核心平台。

## PEFT
PEFT (Parameter-Efficient Fine-Tuning) 是一种高效的微调方法，旨在解决大规模预训练模型微调过程中的参数调整问题。与传统的微调方法不同，PEFT 通过修改模型中的少量参数来实现微调，从而大大减少了计算开销和存储需求。

- 允许在有限的计算资源上进行大规模预训练模型的微调，特别适合资源有限的开发者或研究者。
- 常见的 PEFT 方法包括 LoRA（Low-Rank Adaptation）、Adapter、Prompt Tuning 等。
- 提供了更高效的训练和部署流程，避免了对整个模型进行微调的昂贵成本。

对于需要在大规模预训练模型上进行高效微调的开发者来说，PEFT 提供了一种重要的思路和方法，尤其是在部署和扩展性方面。 

## LangChain
LangChain 是一个用于构建 语言模型驱动的应用程序 的框架，尤其适合 大语言模型（LLM）。它提供了用于集成外部知识库、数据库、API、文档、检索系统等的工具，以增强语言模型的功能，使其能够更好地处理现实世界中的复杂任务。
- 支持与 外部数据源（如数据库、搜索引擎、API）进行交互，结合 文本生成 和 检索机制，实现更强大的推理能力。
- 可以方便地创建 多步骤 和 多任务 的工作流，构建更复杂的应用。
- 支持 语言模型链（Chain）、Agent 等构建方式，能够帮助开发者更灵活地构建对话式应用。

LangChain 在处理 对话式AI 和 多任务协作 的场景中具有重要作用，尤其是在构建 智能助手、自动化问答系统、知识管理系统 时，非常有价值。

因此，学习这三个核心框架，可以让你在 自然语言处理 和 生成式AI 的领域内，快速掌握目前最热门和最有前景的技术，提升开发效率和能力。